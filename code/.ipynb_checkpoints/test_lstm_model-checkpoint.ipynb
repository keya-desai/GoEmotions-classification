{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torch.optim as optim\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size = 100, biDirectional = False):\n",
    "        super(Model, self).__init__() \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        num_classes : 28 = (For full classification)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM  \n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.mlp_out_size = mlp_out_size\n",
    "        self.biDirectional = biDirectional\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.lstm_layer = LSTM(self.batch_size, self.hidden_size, self.embedding_length, self.biDirectional, self.num_layers)\n",
    "\n",
    "        if(self.biDirectional):\n",
    "            self.mlp = MLP(self.hidden_size*2, self.mlp_out_size, self.num_classes)\n",
    "        else:\n",
    "            self.mlp = MLP(self.hidden_size, self.mlp_out_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        input_ = self.word_embeddings(input_sequence)\n",
    "        out_lstm, final_hidden_state = self.lstm_layer(input_)\n",
    "        if self.biDirectional:\n",
    "            final_hidden_state = final_hidden_state.view(self.num_layers, 2, input_.shape[0], self.hidden_size) # num_layer x num_dir x batch x hidden\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "            final_hidden_state = final_hidden_state.transpose(0,1).reshape(input_.shape[0], self.hidden_size*2)\n",
    "        else:\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "        \n",
    "        mlp_output = self.mlp(final_hidden_state)\n",
    "        predictions = torch.softmax(mlp_output, dim = -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        hidden_size : Size of the hidden_state of the LSTM  \n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.biDirectional= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "    def forward(self, input_sequence, batch_size=None):\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "        return out_lstm, final_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim, self.num_classes)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "#         out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = torch.load('models/class_15/vocab')\n",
    "word_embeddings = torch.load('models/class_15/word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change accordingly\n",
    "batch_size = 32\n",
    "num_classes = 15\n",
    "\n",
    "embedding_length = 100\n",
    "mlp_out_size = 64\n",
    "weights = word_embeddings\n",
    "hidden_size = 100\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (word_embeddings): Embedding(38110, 100)\n",
       "  (lstm_layer): LSTM(\n",
       "    (lstm): LSTM(100, 100, num_layers=3, batch_first=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (ff_1): Linear(in_features=100, out_features=64, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (ff_2): Linear(in_features=64, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change model/ path\n",
    "loaded_model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size, biDirectional=False)\n",
    "loaded_model.load_state_dict(torch.load('models/class_15/LSTM_BS_32'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(test_sen):\n",
    "    test_sen_list = TEXT.preprocess(test_sen)\n",
    "    test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
    "    # print(test_sen)\n",
    "\n",
    "    test_sen = np.asarray(test_sen)\n",
    "    test_sen = torch.LongTensor(test_sen)\n",
    "    test_tensor = Variable(test_sen)\n",
    "\n",
    "    loaded_model.eval()\n",
    "    prediction = loaded_model(test_tensor)\n",
    "\n",
    "    out_class = torch.argmax(prediction)\n",
    "    \n",
    "    # Can hardcode class label text instead of index\n",
    "    return out_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "0\n",
      "------------\n",
      "1\n",
      "------------\n",
      "6\n",
      "------------\n",
      "0\n",
      "------------\n",
      "2\n",
      "------------\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "test_sen1 = \"We need more boards and to create a bit more space for [NAME]. Then weâ€™ll be good.\" # Neutral\n",
    "test_sen2 = \"Not surprised, damn that sucks. Concussions are awful.\" # Anger\n",
    "# test_sen3 = \"Are you kidding me!! Really??\"\n",
    "# test_sen3 = \"seriously wtf. I want to see how the whole hand went in detail. that was the sickest soulread ever\" # Anger\n",
    "test_sen3 = \"Thank you SO much! This is so genuine and so helpful. Thank you so much for your time and your thoughts.\"\n",
    "test_sen4 = \"I am so happy for you\" # Joy\n",
    "test_sen5 = \"I do not know what that is.\" # Nuetral\n",
    "test_sen6 = \"Are you kidding me!! Really??\" # Joy\n",
    "\n",
    "test_sen = [test_sen1, test_sen2, test_sen3, test_sen4, test_sen5, test_sen6]\n",
    "\n",
    "for i in range(6):\n",
    "    print('------------')\n",
    "    x = test_sentence(test_sen[i])\n",
    "    print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
