{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torch.optim as optim\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size = 100, biDirectional = False):\n",
    "        super(Model, self).__init__() \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        num_classes : 28 = (For full classification)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.mlp_out_size = mlp_out_size\n",
    "        self.biDirectional = biDirectional\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.lstm_layer = LSTM(self.batch_size, self.hidden_size, self.embedding_length, self.biDirectional, self.num_layers)\n",
    "\n",
    "        if(self.biDirectional):\n",
    "            self.mlp = MLP(self.hidden_size*2, self.mlp_out_size, self.num_classes)\n",
    "#             self.FF = nn.Linear(self.hidden_size*2, num_classes)\n",
    "        else:\n",
    "            self.mlp = MLP(self.hidden_size, self.mlp_out_size, self.num_classes)\n",
    "#             self.FF = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        input_ = self.word_embeddings(input_sequence)\n",
    "        out_lstm, final_hidden_state = self.lstm_layer(input_)\n",
    "        if self.biDirectional:\n",
    "            final_hidden_state = final_hidden_state.view(self.num_layers, 2, input_.shape[0], self.hidden_size) # num_layer x num_dir x batch x hidden\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "            final_hidden_state = final_hidden_state.transpose(0,1).reshape(input_.shape[0], self.hidden_size*2)\n",
    "        else:\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "        \n",
    "        mlp_output = self.mlp(final_hidden_state)\n",
    "#         ff_output = self.FF(mlp_output)\n",
    "#         print(\"FF out size: \", ff_output.shape)\n",
    "        predictions = torch.softmax(mlp_output, dim = -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.biDirectional= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "    def forward(self, input_sequence, batch_size=None):\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "        return out_lstm, final_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to add extra MLP Layer\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim, self.num_classes)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "#         out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_iter, epoch, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    \n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.labels\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size): # One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "#         print(prediction.shape)\n",
    "#         print(prediction)\n",
    "#         print(target.shape)\n",
    "#         print(target)\n",
    "        loss =  loss_fn(prediction, target)\n",
    "#         if math.isnan(loss.item()):\n",
    "#             print(prediction, target)\n",
    "\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_iter, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not batch_size):\n",
    "                continue\n",
    "            target = batch.labels\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            # Sanity check\n",
    "            # print(\"Test Prediction: \", prediction)\n",
    "            # Defualt - Cross entropy loss funtion\n",
    "            loss =  loss_fn(prediction, target)\n",
    "            \n",
    "            if math.isnan(loss.item()):\n",
    "                print(prediction, target)\n",
    "            \n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "            \n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size= 16, embedding_length = 100):\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=30)\n",
    "    LABELS = data.LabelField(batch_first=True, dtype=torch.float)\n",
    "\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "      path='/Users/prakruti/Documents/GoEmotions-classification/data/multi_class_15/', train='train.tsv',\n",
    "      validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "      fields=[('text', TEXT), ('labels', LABELS)])\n",
    "    \n",
    "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "      (train, val, test), batch_sizes=(batch_size, batch_size, batch_size), sort_key=lambda x: len(x.text), device=0)\n",
    "\n",
    "    # build the vocabulary\n",
    "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
    "    LABELS.build_vocab(train)\n",
    "    print(LABELS.vocab.__dict__)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize(x):\n",
    "#     if x == '':\n",
    "#         return 5\n",
    "#     else:\n",
    "#         x = float(x)\n",
    "#         return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({'0': 4771, '1': 4387, '2': 4037, '3': 3181, '4': 3173, '5': 2939, '6': 2662, '7': 2191, '8': 1948, '9': 1716, '10': 1581, '11': 1368, '12': 1110, '13': 1060, '14': 760}), 'itos': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'], 'unk_index': None, 'stoi': defaultdict(None, {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}), 'vectors': None}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(valid_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "learning_rate = 2e-4\n",
    "embedding_length = 100\n",
    "num_classes = 15\n",
    "mlp_out_size = 64\n",
    "weights = word_embeddings\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "\n",
    "model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size, biDirectional=False)\n",
    "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Epoch: 51\n",
      "Train Loss: 2.258, Train Acc: 55.52%\n",
      "Val Loss: 2.423194, Val Acc: 37.24%\n",
      "-------------\n",
      "Epoch 51\n",
      "Epoch: 52\n",
      "Train Loss: 2.253, Train Acc: 55.97%\n",
      "Val Loss: 2.422447, Val Acc: 37.26%\n",
      "-------------\n",
      "Epoch 52\n",
      "Epoch: 53\n",
      "Train Loss: 2.247, Train Acc: 56.63%\n",
      "Val Loss: 2.421535, Val Acc: 37.18%\n",
      "-------------\n",
      "Epoch 53\n",
      "Epoch: 54\n",
      "*** Least validation loss\n",
      "Train Loss: 2.241, Train Acc: 57.30%\n",
      "Val Loss: 2.414491, Val Acc: 37.95%\n",
      "-------------\n",
      "Epoch 54\n",
      "Epoch: 55\n",
      "*** Least validation loss\n",
      "Train Loss: 2.239, Train Acc: 57.40%\n",
      "Val Loss: 2.409064, Val Acc: 38.49%\n",
      "-------------\n",
      "Epoch 55\n",
      "Epoch: 56\n",
      "Train Loss: 2.235, Train Acc: 57.88%\n",
      "Val Loss: 2.417169, Val Acc: 37.76%\n",
      "-------------\n",
      "Epoch 56\n",
      "Epoch: 57\n",
      "*** Least validation loss\n",
      "Train Loss: 2.234, Train Acc: 57.93%\n",
      "Val Loss: 2.408889, Val Acc: 38.81%\n",
      "-------------\n",
      "Epoch 57\n",
      "Epoch: 58\n",
      "Train Loss: 2.236, Train Acc: 57.74%\n",
      "Val Loss: 2.414891, Val Acc: 38.00%\n",
      "-------------\n",
      "Epoch 58\n",
      "Epoch: 59\n",
      "Train Loss: 2.228, Train Acc: 58.57%\n",
      "Val Loss: 2.410243, Val Acc: 38.62%\n",
      "-------------\n",
      "Epoch 59\n",
      "Epoch: 60\n",
      "Train Loss: 2.225, Train Acc: 58.84%\n",
      "Val Loss: 2.409752, Val Acc: 38.47%\n",
      "-------------\n",
      "Epoch 60\n",
      "Epoch: 61\n",
      "*** Least validation loss\n",
      "Train Loss: 2.225, Train Acc: 58.86%\n",
      "Val Loss: 2.399445, Val Acc: 39.61%\n",
      "-------------\n",
      "Epoch 61\n",
      "Epoch: 62\n",
      "Train Loss: 2.222, Train Acc: 59.13%\n",
      "Val Loss: 2.403124, Val Acc: 39.14%\n",
      "-------------\n",
      "Epoch 62\n",
      "Epoch: 63\n",
      "Train Loss: 2.222, Train Acc: 59.19%\n",
      "Val Loss: 2.402727, Val Acc: 39.20%\n",
      "-------------\n",
      "Epoch 63\n",
      "Epoch: 64\n",
      "Train Loss: 2.219, Train Acc: 59.47%\n",
      "Val Loss: 2.405918, Val Acc: 39.03%\n",
      "-------------\n",
      "Epoch 64\n",
      "Epoch: 65\n",
      "Train Loss: 2.216, Train Acc: 59.77%\n",
      "Val Loss: 2.402101, Val Acc: 39.29%\n",
      "-------------\n",
      "Epoch 65\n",
      "Epoch: 66\n",
      "Train Loss: 2.215, Train Acc: 59.83%\n",
      "Val Loss: 2.403359, Val Acc: 39.25%\n",
      "-------------\n",
      "Epoch 66\n",
      "Epoch: 67\n",
      "Train Loss: 2.214, Train Acc: 59.91%\n",
      "Val Loss: 2.402850, Val Acc: 39.14%\n",
      "-------------\n",
      "Epoch 67\n",
      "Epoch: 68\n",
      "*** Least validation loss\n",
      "Train Loss: 2.210, Train Acc: 60.33%\n",
      "Val Loss: 2.396654, Val Acc: 39.76%\n",
      "-------------\n",
      "Epoch 68\n",
      "Epoch: 69\n",
      "Train Loss: 2.210, Train Acc: 60.34%\n",
      "Val Loss: 2.398115, Val Acc: 39.55%\n",
      "-------------\n",
      "Epoch 69\n",
      "Epoch: 70\n",
      "*** Least validation loss\n",
      "Train Loss: 2.208, Train Acc: 60.58%\n",
      "Val Loss: 2.394093, Val Acc: 40.19%\n",
      "-------------\n",
      "Epoch 70\n",
      "Epoch: 71\n",
      "Train Loss: 2.207, Train Acc: 60.65%\n",
      "Val Loss: 2.398445, Val Acc: 39.59%\n",
      "-------------\n",
      "Epoch 71\n",
      "Epoch: 72\n",
      "Train Loss: 2.204, Train Acc: 60.88%\n",
      "Val Loss: 2.394537, Val Acc: 40.22%\n",
      "-------------\n",
      "Epoch 72\n",
      "Epoch: 73\n",
      "Train Loss: 2.203, Train Acc: 60.98%\n",
      "Val Loss: 2.403028, Val Acc: 39.07%\n",
      "-------------\n",
      "Epoch 73\n",
      "Epoch: 74\n",
      "Train Loss: 2.202, Train Acc: 61.19%\n",
      "Val Loss: 2.397352, Val Acc: 39.74%\n",
      "-------------\n",
      "Epoch 74\n",
      "Epoch: 75\n",
      "Train Loss: 2.198, Train Acc: 61.51%\n",
      "Val Loss: 2.396819, Val Acc: 39.61%\n",
      "-------------\n",
      "Epoch 75\n",
      "Epoch: 76\n",
      "Train Loss: 2.197, Train Acc: 61.69%\n",
      "Val Loss: 2.395346, Val Acc: 40.00%\n",
      "-------------\n",
      "Epoch 76\n",
      "Epoch: 77\n",
      "Train Loss: 2.198, Train Acc: 61.62%\n",
      "Val Loss: 2.402952, Val Acc: 39.14%\n",
      "-------------\n",
      "Epoch 77\n",
      "Epoch: 78\n",
      "*** Least validation loss\n",
      "Train Loss: 2.196, Train Acc: 61.71%\n",
      "Val Loss: 2.393737, Val Acc: 40.09%\n",
      "-------------\n",
      "Epoch 78\n",
      "Epoch: 79\n",
      "Train Loss: 2.194, Train Acc: 62.00%\n",
      "Val Loss: 2.394492, Val Acc: 40.02%\n",
      "-------------\n",
      "Epoch 79\n",
      "Epoch: 80\n",
      "Train Loss: 2.193, Train Acc: 62.08%\n",
      "Val Loss: 2.394422, Val Acc: 40.02%\n",
      "-------------\n",
      "Epoch 80\n",
      "Epoch: 81\n",
      "*** Least validation loss\n",
      "Train Loss: 2.192, Train Acc: 62.18%\n",
      "Val Loss: 2.390002, Val Acc: 40.47%\n",
      "-------------\n",
      "Epoch 81\n",
      "Epoch: 82\n",
      "Train Loss: 2.194, Train Acc: 61.94%\n",
      "Val Loss: 2.398491, Val Acc: 39.68%\n",
      "-------------\n",
      "Epoch 82\n",
      "Epoch: 83\n",
      "Train Loss: 2.191, Train Acc: 62.28%\n",
      "Val Loss: 2.392099, Val Acc: 40.24%\n",
      "-------------\n",
      "Epoch 83\n",
      "Epoch: 84\n",
      "Train Loss: 2.190, Train Acc: 62.30%\n",
      "Val Loss: 2.397624, Val Acc: 39.66%\n",
      "-------------\n",
      "Epoch 84\n",
      "Epoch: 85\n",
      "Train Loss: 2.189, Train Acc: 62.42%\n",
      "Val Loss: 2.398394, Val Acc: 39.55%\n",
      "-------------\n",
      "Epoch 85\n",
      "Epoch: 86\n",
      "Train Loss: 2.188, Train Acc: 62.58%\n",
      "Val Loss: 2.397611, Val Acc: 39.76%\n",
      "-------------\n",
      "Epoch 86\n",
      "Epoch: 87\n",
      "*** Least validation loss\n",
      "Train Loss: 2.186, Train Acc: 62.80%\n",
      "Val Loss: 2.387305, Val Acc: 40.88%\n",
      "-------------\n",
      "Epoch 87\n",
      "Epoch: 88\n",
      "Train Loss: 2.185, Train Acc: 62.83%\n",
      "Val Loss: 2.391355, Val Acc: 40.45%\n",
      "-------------\n",
      "Epoch 88\n",
      "Epoch: 89\n",
      "Train Loss: 2.184, Train Acc: 62.96%\n",
      "Val Loss: 2.393559, Val Acc: 40.13%\n",
      "-------------\n",
      "Epoch 89\n",
      "Epoch: 90\n",
      "Train Loss: 2.181, Train Acc: 63.19%\n",
      "Val Loss: 2.388303, Val Acc: 40.62%\n",
      "-------------\n",
      "Epoch 90\n",
      "Epoch: 91\n",
      "Train Loss: 2.182, Train Acc: 63.13%\n",
      "Val Loss: 2.393968, Val Acc: 40.02%\n",
      "-------------\n",
      "Epoch 91\n",
      "Epoch: 92\n",
      "*** Least validation loss\n",
      "Train Loss: 2.180, Train Acc: 63.34%\n",
      "Val Loss: 2.384665, Val Acc: 41.01%\n",
      "-------------\n",
      "Epoch 92\n",
      "Epoch: 93\n",
      "Train Loss: 2.180, Train Acc: 63.36%\n",
      "Val Loss: 2.385157, Val Acc: 40.91%\n",
      "-------------\n",
      "Epoch 93\n",
      "Epoch: 94\n",
      "Train Loss: 2.177, Train Acc: 63.59%\n",
      "Val Loss: 2.386416, Val Acc: 40.69%\n",
      "-------------\n",
      "Epoch 94\n"
     ]
    }
   ],
   "source": [
    "# num_bad_epochs = 0\n",
    "# epoch = 0\n",
    "# least_loss = float('inf')\n",
    "# training_stats = pd.DataFrame(columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc'])\n",
    "\n",
    "while(True):\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_loss, train_acc = train_model(model, optim, train_iter, epoch, batch_size, num_classes)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter, batch_size, num_classes) \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    if val_loss < least_loss:\n",
    "        least_loss = val_loss\n",
    "        num_bad_epochs = 0\n",
    "        print(\"*** Least validation loss\")\n",
    "        torch.save(model.state_dict(), \"models/class_15/LSTM_BS_32\")\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "#     print(f'Epoch: {epoch+1:2}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%') \n",
    "    print(f'Val Loss: {val_loss:3f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    training_stats = training_stats.append(\n",
    "        pd.Series([epoch+1, train_loss, train_acc, val_loss, val_acc], index=training_stats.columns), \n",
    "        ignore_index=True)\n",
    "    if num_bad_epochs >= 8:\n",
    "        break\n",
    "        \n",
    "    epoch += 1\n",
    "    if epoch == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0181e2758004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_stats/class_15/LSTM_BS_32.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_stats' is not defined"
     ]
    }
   ],
   "source": [
    "training_stats.to_csv('training_stats/class_15/LSTM_BS_32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (word_embeddings): Embedding(38110, 100)\n",
       "  (lstm_layer): LSTM(\n",
       "    (lstm): LSTM(100, 100, num_layers=3, batch_first=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (ff_1): Linear(in_features=100, out_features=64, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (ff_2): Linear(in_features=64, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights,num_layers, hidden_size, biDirectional=False)\n",
    "loaded_model.load_state_dict(torch.load('models/class_15/LSTM_BS_32'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.384, Test Acc: 41.07\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_model(loaded_model, test_iter, batch_size, num_classes)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(test_sen):\n",
    "    test_sen_list = TEXT.preprocess(test_sen)\n",
    "    print(test_sen_list)\n",
    "    test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
    "    # print(test_sen)\n",
    "\n",
    "    test_sen = np.asarray(test_sen)\n",
    "    test_sen = torch.LongTensor(test_sen)\n",
    "    test_tensor = Variable(test_sen)\n",
    "\n",
    "    # print(test_tensor)\n",
    "    loaded_model.eval()\n",
    "#     print(test_tensor.shape)\n",
    "    prediction = loaded_model(test_tensor)\n",
    "    print(\"prediction =\", prediction)\n",
    "\n",
    "    out_class = torch.argmax(prediction)\n",
    "    return out_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_idx = {\n",
    "#     '0' :  '0', # admiration, desire\n",
    "#     '10' :  '1', # disapproval, disgust, disappointment, embarrassment\n",
    "#     '2' : '2',  # anger, annoyance\n",
    "#     '13' : '3', # excitement, amusement\n",
    "#     '18' : '4', # love, caring\n",
    "#     '4' : '5',  # approval\n",
    "#     '15' : '6', # gratitude\n",
    "#     '7' : '7',  # curiosity\n",
    "#     '25' : '8', # sadness , grief, remorse\n",
    "#     '17' : '9', # joy , pride, relief\n",
    "#     '20' : '10', # optimism\n",
    "#     '6' : '11', # confusion\n",
    "#     '22' : '12', # realization\n",
    "#     '26' : '13', # surprise\n",
    "#     '14' :  '14' # fear, nervousness\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "['we', 'need', 'more', 'boards', 'and', 'to', 'create', 'a', 'bit', 'more', 'space', 'for', '[name].', 'then', 'we’ll', 'be', 'good.']\n",
      "prediction = tensor([[9.9853e-01, 3.2671e-10, 5.1728e-11, 7.2792e-10, 1.3660e-03, 9.4108e-16,\n",
      "         1.1968e-07, 6.2128e-13, 1.4500e-19, 1.0028e-18, 1.0177e-04, 6.5502e-21,\n",
      "         5.3880e-22, 2.4030e-21, 1.1633e-22]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0)\n",
      "------------\n",
      "['not', 'surprised,', 'damn', 'that', 'sucks.', 'concussions', 'are', 'awful.']\n",
      "prediction = tensor([[3.2741e-15, 1.0000e+00, 6.7946e-14, 7.6872e-15, 1.7044e-22, 1.6645e-24,\n",
      "         2.4135e-20, 9.7690e-20, 4.1407e-17, 1.1525e-24, 2.9055e-24, 6.3256e-27,\n",
      "         4.9868e-28, 2.8914e-28, 1.8674e-28]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(1)\n",
      "------------\n",
      "['thank', 'you', 'so', 'much!', 'this', 'is', 'so', 'genuine', 'and', 'so', 'helpful.', 'thank', 'you', 'so', 'much', 'for', 'your', 'time', 'and', 'your', 'thoughts.']\n",
      "prediction = tensor([[6.6895e-24, 7.1446e-33, 4.7108e-27, 1.2531e-23, 2.0055e-35, 1.8136e-36,\n",
      "         1.0000e+00, 4.9177e-20, 5.2536e-37, 6.4702e-35, 2.1719e-26, 3.9376e-41,\n",
      "         6.5721e-43, 2.1637e-41, 1.2470e-40]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(6)\n",
      "------------\n",
      "['i', 'am', 'so', 'happy', 'for', 'you']\n",
      "prediction = tensor([[1.0000e+00, 1.2975e-11, 2.1345e-12, 9.8306e-09, 7.5624e-11, 6.7373e-17,\n",
      "         7.0772e-09, 8.0659e-13, 1.0965e-16, 6.8339e-20, 8.8680e-07, 2.4034e-22,\n",
      "         9.9157e-24, 3.9815e-23, 4.4552e-24]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0)\n",
      "------------\n",
      "['i', 'do', 'not', 'know', 'what', 'that', 'is.']\n",
      "prediction = tensor([[4.3660e-06, 8.9185e-02, 9.1072e-01, 1.1118e-05, 2.7419e-13, 1.0040e-10,\n",
      "         4.7645e-08, 4.3246e-05, 3.4240e-05, 3.9207e-11, 4.9222e-08, 7.0486e-13,\n",
      "         5.9140e-14, 1.2035e-13, 3.4629e-14]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(2)\n",
      "------------\n",
      "['are', 'you', 'kidding', 'me!!', 'really??']\n",
      "prediction = tensor([[3.7493e-05, 2.0075e-06, 5.0633e-05, 1.8701e-02, 2.6293e-21, 2.6112e-14,\n",
      "         3.2242e-05, 9.8118e-01, 1.1757e-08, 1.0857e-15, 2.6699e-09, 9.6104e-19,\n",
      "         2.6229e-21, 2.0991e-20, 3.7851e-20]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# test_sen1 = \"I enjoyed it.\"\n",
    "test_sen1 = \"We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\" # Neutral\n",
    "test_sen2 = \"Not surprised, damn that sucks. Concussions are awful.\" # Anger\n",
    "# test_sen3 = \"Are you kidding me!! Really??\"\n",
    "# test_sen3 = \"seriously wtf. I want to see how the whole hand went in detail. that was the sickest soulread ever\" # Anger\n",
    "test_sen3 = \"Thank you SO much! This is so genuine and so helpful. Thank you so much for your time and your thoughts.\"\n",
    "test_sen4 = \"I am so happy for you\" # Joy\n",
    "test_sen5 = \"I do not know what that is.\" # Nuetral\n",
    "test_sen6 = \"Are you kidding me!! Really??\" # Joy\n",
    "\n",
    "test_sen = [test_sen1, test_sen2, test_sen3, test_sen4, test_sen5, test_sen6]\n",
    "\n",
    "for i in range(6):\n",
    "    print('------------')\n",
    "    x = test_sentence(test_sen[i])\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
