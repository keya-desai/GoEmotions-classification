{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.4\n",
      "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (4.47.0)\n",
      "Requirement already satisfied: requests in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (2.24.0)\n",
      "Requirement already satisfied: six in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (1.15.0)\n",
      "Collecting torch\n",
      "  Downloading torch-1.7.0-cp38-none-macosx_10_9_x86_64.whl (108.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 108.1 MB 23.8 MB/s eta 0:00:01   |███                             | 10.5 MB 3.0 MB/s eta 0:00:33\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from torchtext==0.4) (1.18.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext==0.4) (1.25.9)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.4) (3.7.4.2)\n",
      "Requirement already satisfied: future in /Users/prakruti/opt/anaconda3/lib/python3.8/site-packages (from torch->torchtext==0.4) (0.18.2)\n",
      "Installing collected packages: dataclasses, torch, torchtext\n",
      "Successfully installed dataclasses-0.6 torch-1.7.0 torchtext-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torch.optim as optim\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, hidden_size = 100, biDirectional = False):\n",
    "        super(Model, self).__init__() \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 28 = (For full classification)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.mlp_out_size = mlp_out_size\n",
    "        self.biDirectional = biDirectional\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.lstm_layer = LSTM(self.batch_size, self.hidden_size, self.embedding_length, self.biDirectional)\n",
    "\n",
    "        if(self.biDirectional):\n",
    "            self.mlp = MLP(self.hidden_size*2, self.mlp_out_size)\n",
    "            self.FF = nn.Linear(self.hidden_size*2, num_classes)\n",
    "        else:\n",
    "            self.mlp = MLP(self.hidden_size, self.mlp_out_size)\n",
    "            self.FF = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        input_ = self.word_embeddings(input_sequence)\n",
    "        out_lstm, final_hidden_state = self.lstm_layer(input_)\n",
    "        if self.biDirectional:\n",
    "            final_hidden_state = final_hidden_state.view(2, 2, input_.shape[0], self.hidden_size) # num_layer x num_dir x batch x hidden\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "            final_hidden_state = final_hidden_state.transpose(0,1).reshape(input_.shape[0], self.hidden_size*2)\n",
    "        else:\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "        \n",
    "        ff_output = self.FF(final_hidden_state)\n",
    "#         print(\"FF out size: \", ff_output.shape)\n",
    "#         predictions = torch.softmax(ff_output, dim = -1)\n",
    "        return ff_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.biDirectional\t= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "    def forward(self, input_sequence, batch_size=None):\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "        return out_lstm, final_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to add extra MLP Layer\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "        out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_iter, epoch, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    \n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.labels\n",
    "        target = torch.autograd.Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size): # One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        output = model(text)\n",
    "#         print(\"prediction = \", prediction.shape)\n",
    "#         print(\"target = \", target.shape)\n",
    "#         print(\"prediction = \", prediction)\n",
    "#         print(\"target = \", target)\n",
    "\n",
    "        # Sigmoid layer and the BCELoss in one single class\n",
    "        loss = loss_fn(output, target.type_as(output))\n",
    "        print(\"loss = \", loss)\n",
    "        \n",
    "        score = torch.sigmoid(output).cpu()       \n",
    "        predicted = torch.round(score)\n",
    "        num_corrects = (predicted == target).sum()\n",
    "        \n",
    "#         num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)*num_classes\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_iter, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    total_attention =  0\n",
    "    total_samples = 0 \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] != 32):\n",
    "                continue\n",
    "            target = batch.labels\n",
    "            target = torch.autograd.Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(text)\n",
    "            \n",
    "            score = torch.sigmoid(output).cpu()       \n",
    "            predicted = torch.round(score)\n",
    "            total += target.size(0)\n",
    "            num_corrects = (predicted == target).sum()\n",
    "            # print(\"Test Prediction: \", prediction)\n",
    "\n",
    "            loss =  loss_fn(output, target.type_as(output))\n",
    "            \n",
    "            if math.isnan(loss.item()):\n",
    "                print(output, target)\n",
    "            \n",
    "#             num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)*num_classes\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "            \n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x):\n",
    "    vec = np.zeros(28)\n",
    "    labels = x.split(',')\n",
    "    for x in labels:\n",
    "        vec[int(x)-1] = 1.0\n",
    "    return vec\n",
    "\n",
    "def load_data(batch_size= 16, embedding_length = 100):\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=30)\n",
    "    LABELS = data.Field(sequential=True, tokenize=vectorize, batch_first=True)\n",
    "\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "      path='/Users/prakruti/Documents/GoEmotions-classification/data', train='train.tsv',\n",
    "      validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "      fields=[('text', TEXT), ('labels', LABELS)])\n",
    "    \n",
    "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "      (train, val, test), batch_sizes=(16, 16, 16),\n",
    "      sort_key=lambda x: len(x.text), device=0)\n",
    "\n",
    "    # build the vocabulary\n",
    "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
    "    LABELS.build_vocab(train)\n",
    "    print(LABELS.vocab.__dict__)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({0.0: 1164377, 1.0: 51103}), 'itos': ['<unk>', '<pad>', 0.0, 1.0], 'unk_index': 0, 'stoi': defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fc0c5443fa0>>, {'<unk>': 0, '<pad>': 1, 0.0: 2, 1.0: 3}), 'vectors': None}\n"
     ]
    }
   ],
   "source": [
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 16]\n",
       "\t[.text]:('[torch.LongTensor of size 16x30]', '[torch.LongTensor of size 16]')\n",
       "\t[.labels]:[torch.LongTensor of size 16x28]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-writing the loss function to simple cross entropy loss\n",
    "loss_fn = loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "learning_rate = 2e-4\n",
    "batch_size = 16\n",
    "output_size = 2\n",
    "embedding_length = 100\n",
    "num_classes = 28\n",
    "mlp_out_size = 32\n",
    "weights = word_embeddings\n",
    "hidden_size = 100\n",
    "\n",
    "model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, hidden_size, biDirectional=False)\n",
    "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(0.7231, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.6778, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.6307, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.5804, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.5088, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.4231, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.3265, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.1792, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(0.0183, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-0.1099, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-0.2712, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-0.4982, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-0.6723, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-0.8416, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-1.0261, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-1.1774, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-1.3775, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-1.5015, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-1.6998, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-1.8899, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.0346, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.2132, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.3769, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.5269, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.6823, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.8316, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-2.9641, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.1359, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.2654, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.4120, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.5381, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.6727, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.8096, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-3.9540, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.0818, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.2086, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.3574, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.4594, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.5961, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.7281, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.8979, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-4.9919, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.0872, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.2459, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.3460, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.4647, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.6124, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.7246, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.8607, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-5.9748, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.0999, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.2122, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.3254, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.4840, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.5433, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.6853, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.7766, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-6.9584, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.0457, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.1414, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.2595, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.4053, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.5180, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.5935, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.7578, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.8156, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-7.9663, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.1657, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.1848, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.2798, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.4507, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.5655, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.6839, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.7703, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.8805, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-8.9936, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.1485, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.2310, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.3693, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.5182, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.5801, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.8227, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-9.9528, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.0097, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.1704, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.2591, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.3318, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.4597, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.5722, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.7133, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.8485, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-10.8902, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.0668, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.1166, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.3349, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.3559, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.5999, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.6241, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.7307, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.8359, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-11.8939, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.0697, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.1608, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.3496, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.3733, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.4703, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.6601, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.8446, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.8277, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-12.9997, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.0385, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.1308, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.2478, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.4657, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(-13.5196, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.6839, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.6767, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.8028, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-13.9158, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.0591, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.1500, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.2468, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.3324, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.4851, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.6605, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.6391, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.7834, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-14.8837, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.0006, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.2025, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.2478, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.4525, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.4695, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.5556, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.7936, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.9080, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-15.9526, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.0762, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.1259, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.3937, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.2997, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.4541, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.6301, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.6390, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.7984, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.1187, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-16.9571, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.1568, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.1835, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.3247, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.3927, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.5676, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.5761, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.8074, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-17.9026, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.0087, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.1252, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.1570, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.3759, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.4189, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.6032, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.6139, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.7864, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-18.7734, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.0854, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.1453, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.1297, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.2758, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.4876, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.5508, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.7273, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.7593, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.9375, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-19.8452, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.0922, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.1927, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.2712, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.4164, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.6168, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.5723, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.7370, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.7966, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.9566, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-20.9691, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.1718, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.1867, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.2351, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.5375, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.5515, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.8577, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.9462, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-21.9008, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.0013, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.0651, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.0962, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.4247, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.3668, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.6070, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.7804, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-22.8584, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.0075, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.0062, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.1684, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.1267, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.4472, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.2875, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.4494, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.6595, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.8110, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.0793, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-23.9230, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.0788, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.0761, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.3389, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.4663, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.4621, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.5762, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.6908, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.8349, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-24.8399, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.1142, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.1625, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.2242, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.3844, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.5984, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.5491, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.7101, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-25.9344, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(-26.0787, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.0313, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.3543, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.0707, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.2550, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.5166, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.6366, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.6809, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.7165, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.8222, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-26.9969, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.0029, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.2812, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.2645, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.3049, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.4208, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.7605, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.8733, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.9181, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-27.9139, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.1961, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.1823, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.2363, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.2751, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.5010, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.6683, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.7811, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.6539, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.8169, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-28.9946, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.0276, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.2652, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.4752, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.3992, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.5256, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.6675, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.8052, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-29.9707, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.0093, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.1240, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.1506, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.3243, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.4452, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.5579, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.6454, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.8112, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.7980, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-30.9162, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.1476, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.1119, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.2273, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.4109, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.6298, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.4862, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.7373, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.8257, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.0100, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-31.9138, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.1468, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.2734, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.2343, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.5545, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.5075, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.7601, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.8679, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.8556, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-32.9518, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.1188, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.2795, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.3413, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.4338, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.6361, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.7230, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.8325, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.8553, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.9103, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-33.9356, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.2612, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.3693, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.6141, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.6610, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.6300, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.5732, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.6827, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.9313, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-34.9601, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.0110, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.1023, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.1288, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.5498, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.5644, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.8999, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.7770, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.9099, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-35.8401, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.2017, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.2263, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.3300, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.5840, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.3793, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.8691, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.6789, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.7985, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-36.9681, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.1526, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.0976, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.1984, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.3115, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.3551, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.6800, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.6421, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.9134, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-37.8376, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.1796, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(-38.2280, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.3341, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.1070, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.3057, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.6576, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.8447, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.5336, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-38.7371, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.1727, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.0266, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.1284, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.5797, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.5869, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.4475, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.5654, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.9251, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.8342, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.7071, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.8859, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-39.9995, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.0859, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.4613, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.5693, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.4206, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.7896, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.6197, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.2587, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.0960, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-40.9648, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.1498, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.4331, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.2692, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.5510, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.4753, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.1240, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.7655, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-41.9796, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.0859, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.0958, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.3787, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.1436, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.4300, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.9736, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.7167, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-42.6480, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.0192, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.3269, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.2494, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.1732, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.1789, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.6626, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.4903, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.6847, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.7034, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.7887, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.9877, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-43.9174, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.2276, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.2273, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.3351, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.4566, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.5454, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.7444, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.0574, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-44.7701, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.5680, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.2651, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.2816, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.3947, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.4975, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.6096, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.0032, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.8120, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-45.9219, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.1295, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.1288, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.2279, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.3515, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.2569, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.2672, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.3628, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.5763, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.8809, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.6761, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-46.9923, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.4907, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.0959, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.3192, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.5101, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.5226, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.5230, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.9247, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.0485, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-47.9446, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.4747, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.5798, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.8824, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.4670, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.2684, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.4683, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.7970, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.1055, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-48.8870, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.0956, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.4196, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.1041, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.6285, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.5329, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.2946, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.9470, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.7514, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-49.9820, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.1795, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.0723, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(-50.3948, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.7142, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.3947, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.4932, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.7061, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.5903, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.9284, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-50.8107, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.0169, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.0320, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.3383, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.1133, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.6794, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.5622, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.8772, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-51.5457, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.0936, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.0959, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.4211, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.3078, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.3849, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.4026, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.8513, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.5040, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.8405, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-52.7098, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.2708, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.0232, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.1578, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.9405, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.5836, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.5659, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.7955, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.6665, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.3405, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.8722, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-53.9815, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.3113, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.3273, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.6575, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.8814, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.3923, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.7473, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.6094, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.7291, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.3035, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-54.9242, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.3833, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.7230, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.8481, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.4745, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.9436, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.4544, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.5449, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.2596, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.3753, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-55.9953, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.1017, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.4374, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.4251, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.7920, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.6446, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.2372, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-56.8477, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.0844, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.1841, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.3957, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.5245, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.3929, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.9873, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.0913, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.4623, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.8063, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-57.9086, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.4004, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.3585, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.4742, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.4513, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.4488, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.4219, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.7672, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.1172, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-58.9842, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.5922, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.3137, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.0586, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.5433, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.4013, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.2583, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.7478, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.9766, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.2055, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.4493, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-59.8857, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.1330, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.3718, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.2072, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.1867, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.8229, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.4053, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.5031, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.7413, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.1123, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-60.9578, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.4528, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.4288, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.5346, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.3710, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.7338, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.9668, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-61.8407, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.0715, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.1776, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(-62.5443, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.1049, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.2254, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.4571, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.5582, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.9395, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.1608, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-62.7652, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.5047, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.3646, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.4741, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.4565, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.8171, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.5187, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.1731, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.7284, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.1012, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-63.7948, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.3276, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.4331, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.5298, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.3804, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.3451, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.5675, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.6823, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-64.9280, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.0364, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.1193, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.3947, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.3437, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.3083, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.1199, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.7988, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.9097, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.1629, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.9649, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-65.9677, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.4764, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.4422, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.1204, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.3740, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.3179, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.8751, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.2846, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.9405, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.9199, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.2998, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-66.9658, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.5027, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.6102, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.7349, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.6755, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.0719, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.7453, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.8486, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-67.9707, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.2074, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.3125, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.5622, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.3600, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.7673, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.5828, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-68.6785, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.1016, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.0502, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.0217, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.1224, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.2354, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.0343, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.2906, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.6970, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.9422, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.1987, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.1576, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.9702, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-69.9264, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.1780, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.5865, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.6991, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.3495, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.6018, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.5507, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-70.8166, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.0761, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.6315, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.2835, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.3887, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.0326, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.6049, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.3818, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.6451, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.0729, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-71.5568, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.1315, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.0741, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.1864, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.3018, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.0907, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.4933, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.0769, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.3368, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.6631, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-72.9191, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.0200, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.1318, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.7030, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.3423, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.2998, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.7130, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.6479, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.7725, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.6436, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-73.8081, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.2467, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.3342, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  tensor(-74.1330, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.8715, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.5058, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.6160, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.0405, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.2983, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.5872, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.5071, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-74.9786, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.8951, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.3329, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.2833, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.7098, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.3144, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.6099, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.8477, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.2994, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-75.9161, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.0399, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.9425, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.3975, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.5125, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.7751, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.5676, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.6639, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-76.9318, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.7034, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.1478, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.9154, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.6984, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.2842, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.5810, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.0117, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.0915, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.7261, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-77.9711, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.2596, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.5096, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.7997, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.2574, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.3486, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.6248, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.5561, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.8348, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-78.7619, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-79.3865, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-79.3108, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "loss =  tensor(-79.2669, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-f91828dba651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-5a61e89187d1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optim, train_iter, epoch, batch_size, num_classes)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#         num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_corrects\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mclip_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_bad_epochs = 0\n",
    "epoch = 0\n",
    "least_loss = float('inf')\n",
    "training_stats = pd.DataFrame(columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc'])\n",
    "\n",
    "while(True):\n",
    "    train_loss, train_acc = train_model(model, optim, train_iter, epoch, batch_size, num_classes)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter, num_classes) \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    if val_loss < least_loss:\n",
    "        least_loss = val_loss\n",
    "        num_bad_epochs = 0\n",
    "        print(\"*** Least validation loss\")\n",
    "        torch.save(model.state_dict(), \"LSTM\")\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "#     print(f'Epoch: {epoch+1:2}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%') \n",
    "    print(f'Val Loss: {val_loss:3f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    training_stats = training_stats.append(\n",
    "        pd.Series([epoch+1, train_loss, train_acc, val_loss, val_acc], index=training_stats.columns), \n",
    "        ignore_index=True)\n",
    "    if num_bad_epochs >= 10:\n",
    "        break\n",
    "        \n",
    "    epoch += 1\n",
    "    if epoch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
