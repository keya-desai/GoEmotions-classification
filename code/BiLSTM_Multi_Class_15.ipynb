{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torch.optim as optim\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size = 100, biDirectional = False):\n",
    "        super(Model, self).__init__() \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        num_classes : 28 = (For full classification)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.mlp_out_size = mlp_out_size\n",
    "        self.biDirectional = biDirectional\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.lstm_layer = LSTM(self.batch_size, self.hidden_size, self.embedding_length, self.biDirectional, self.num_layers)\n",
    "\n",
    "        if(self.biDirectional):\n",
    "            self.mlp = MLP(self.hidden_size*2, self.mlp_out_size, self.num_classes)\n",
    "#             self.FF = nn.Linear(self.hidden_size*2, num_classes)\n",
    "        else:\n",
    "            self.mlp = MLP(self.hidden_size, self.mlp_out_size, self.num_classes)\n",
    "#             self.FF = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        input_ = self.word_embeddings(input_sequence)\n",
    "        out_lstm, final_hidden_state = self.lstm_layer(input_)\n",
    "        if self.biDirectional:\n",
    "            final_hidden_state = final_hidden_state.view(self.num_layers, 2, input_.shape[0], self.hidden_size) # num_layer x num_dir x batch x hidden\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "            final_hidden_state = final_hidden_state.transpose(0,1).reshape(input_.shape[0], self.hidden_size*2)\n",
    "        else:\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "        \n",
    "        mlp_output = self.mlp(final_hidden_state)\n",
    "#         ff_output = self.FF(mlp_output)\n",
    "#         print(\"FF out size: \", ff_output.shape)\n",
    "        predictions = torch.softmax(mlp_output, dim = -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.biDirectional= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "    def forward(self, input_sequence, batch_size=None):\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "        return out_lstm, final_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to add extra MLP Layer\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim, self.num_classes)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "#         out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_iter, epoch, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    \n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.labels\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size): # One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "#         print(prediction.shape)\n",
    "#         print(prediction)\n",
    "#         print(target.shape)\n",
    "#         print(target)\n",
    "        loss =  loss_fn(prediction, target)\n",
    "#         if math.isnan(loss.item()):\n",
    "#             print(prediction, target)\n",
    "\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_iter, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not batch_size):\n",
    "                continue\n",
    "            target = batch.labels\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            # Sanity check\n",
    "            # print(\"Test Prediction: \", prediction)\n",
    "            # Defualt - Cross entropy loss funtion\n",
    "            loss =  loss_fn(prediction, target)\n",
    "            \n",
    "            if math.isnan(loss.item()):\n",
    "                print(prediction, target)\n",
    "            \n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "            \n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size= 16, embedding_length = 100):\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=30)\n",
    "    LABELS = data.LabelField(batch_first=True, dtype=torch.float)\n",
    "\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "      path='/Users/prakruti/Documents/GoEmotions-classification/data/multi_class_15/', train='train.tsv',\n",
    "      validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "      fields=[('text', TEXT), ('labels', LABELS)])\n",
    "    \n",
    "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "      (train, val, test), batch_sizes=(batch_size, batch_size, batch_size), sort_key=lambda x: len(x.text), device=0)\n",
    "\n",
    "    # build the vocabulary\n",
    "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
    "    LABELS.build_vocab(train)\n",
    "    print(LABELS.vocab.__dict__)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize(x):\n",
    "#     if x == '':\n",
    "#         return 5\n",
    "#     else:\n",
    "#         x = float(x)\n",
    "#         return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({'0': 4771, '1': 4387, '2': 4037, '3': 3181, '4': 3173, '5': 2939, '6': 2662, '7': 2191, '8': 1948, '9': 1716, '10': 1581, '11': 1368, '12': 1110, '13': 1060, '14': 760}), 'itos': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'], 'unk_index': None, 'stoi': defaultdict(None, {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}), 'vectors': None}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab_size, 'models/class_15/BiLSTM_vocab')\n",
    "torch.save(word_embeddings, 'models/class_15/BiLSTM_word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "learning_rate = 2e-4\n",
    "embedding_length = 100\n",
    "num_classes = 15\n",
    "mlp_out_size = 64\n",
    "weights = word_embeddings\n",
    "hidden_size = 100\n",
    "num_layers = 3\n",
    "\n",
    "model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size, biDirectional=True)\n",
    "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch: 01\n",
      "*** Least validation loss\n",
      "Train Loss: 2.642, Train Acc: 15.93%\n",
      "Val Loss: 2.616884, Val Acc: 17.35%\n",
      "-------------\n",
      "Epoch 1\n",
      "Epoch: 02\n",
      "*** Least validation loss\n",
      "Train Loss: 2.610, Train Acc: 19.67%\n",
      "Val Loss: 2.562409, Val Acc: 23.17%\n",
      "-------------\n",
      "Epoch 2\n",
      "Epoch: 03\n",
      "*** Least validation loss\n",
      "Train Loss: 2.546, Train Acc: 26.36%\n",
      "Val Loss: 2.520485, Val Acc: 27.33%\n",
      "-------------\n",
      "Epoch 3\n",
      "Epoch: 04\n",
      "*** Least validation loss\n",
      "Train Loss: 2.513, Train Acc: 29.81%\n",
      "Val Loss: 2.503529, Val Acc: 29.12%\n",
      "-------------\n",
      "Epoch 4\n",
      "Epoch: 05\n",
      "Train Loss: 2.479, Train Acc: 33.36%\n",
      "Val Loss: 2.524757, Val Acc: 27.00%\n",
      "-------------\n",
      "Epoch 5\n",
      "Epoch: 06\n",
      "*** Least validation loss\n",
      "Train Loss: 2.453, Train Acc: 35.90%\n",
      "Val Loss: 2.475300, Val Acc: 31.68%\n",
      "-------------\n",
      "Epoch 6\n",
      "Epoch: 07\n",
      "*** Least validation loss\n",
      "Train Loss: 2.431, Train Acc: 38.06%\n",
      "Val Loss: 2.449828, Val Acc: 34.53%\n",
      "-------------\n",
      "Epoch 7\n",
      "Epoch: 08\n",
      "*** Least validation loss\n",
      "Train Loss: 2.413, Train Acc: 39.89%\n",
      "Val Loss: 2.448612, Val Acc: 34.44%\n",
      "-------------\n",
      "Epoch 8\n",
      "Epoch: 09\n",
      "*** Least validation loss\n",
      "Train Loss: 2.397, Train Acc: 41.58%\n",
      "Val Loss: 2.440951, Val Acc: 35.32%\n",
      "-------------\n",
      "Epoch 9\n",
      "Epoch: 10\n",
      "*** Least validation loss\n",
      "Train Loss: 2.382, Train Acc: 43.08%\n",
      "Val Loss: 2.433673, Val Acc: 36.12%\n",
      "-------------\n",
      "Epoch 10\n",
      "Epoch: 11\n",
      "*** Least validation loss\n",
      "Train Loss: 2.368, Train Acc: 44.44%\n",
      "Val Loss: 2.428084, Val Acc: 36.68%\n",
      "-------------\n",
      "Epoch 11\n",
      "Epoch: 12\n",
      "*** Least validation loss\n",
      "Train Loss: 2.352, Train Acc: 46.05%\n",
      "Val Loss: 2.418156, Val Acc: 37.69%\n",
      "-------------\n",
      "Epoch 12\n",
      "Epoch: 13\n",
      "Train Loss: 2.342, Train Acc: 47.09%\n",
      "Val Loss: 2.419718, Val Acc: 37.46%\n",
      "-------------\n",
      "Epoch 13\n",
      "Epoch: 14\n",
      "*** Least validation loss\n",
      "Train Loss: 2.337, Train Acc: 47.65%\n",
      "Val Loss: 2.418125, Val Acc: 37.65%\n",
      "-------------\n",
      "Epoch 14\n",
      "Epoch: 15\n",
      "*** Least validation loss\n",
      "Train Loss: 2.332, Train Acc: 48.16%\n",
      "Val Loss: 2.411026, Val Acc: 38.36%\n",
      "-------------\n",
      "Epoch 15\n",
      "Epoch: 16\n",
      "*** Least validation loss\n",
      "Train Loss: 2.318, Train Acc: 49.54%\n",
      "Val Loss: 2.403339, Val Acc: 38.99%\n",
      "-------------\n",
      "Epoch 16\n",
      "Epoch: 17\n",
      "*** Least validation loss\n",
      "Train Loss: 2.307, Train Acc: 50.56%\n",
      "Val Loss: 2.401802, Val Acc: 39.27%\n",
      "-------------\n",
      "Epoch 17\n",
      "Epoch: 18\n",
      "*** Least validation loss\n",
      "Train Loss: 2.298, Train Acc: 51.44%\n",
      "Val Loss: 2.400990, Val Acc: 39.63%\n",
      "-------------\n",
      "Epoch 18\n",
      "Epoch: 19\n",
      "*** Least validation loss\n",
      "Train Loss: 2.292, Train Acc: 52.08%\n",
      "Val Loss: 2.398411, Val Acc: 39.68%\n",
      "-------------\n",
      "Epoch 19\n",
      "Epoch: 20\n",
      "*** Least validation loss\n",
      "Train Loss: 2.286, Train Acc: 52.70%\n",
      "Val Loss: 2.394574, Val Acc: 40.09%\n",
      "-------------\n",
      "Epoch 20\n",
      "Epoch: 21\n",
      "*** Least validation loss\n",
      "Train Loss: 2.282, Train Acc: 53.11%\n",
      "Val Loss: 2.394058, Val Acc: 40.15%\n",
      "-------------\n",
      "Epoch 21\n",
      "Epoch: 22\n",
      "*** Least validation loss\n",
      "Train Loss: 2.278, Train Acc: 53.52%\n",
      "Val Loss: 2.393418, Val Acc: 40.24%\n",
      "-------------\n",
      "Epoch 22\n",
      "Epoch: 23\n",
      "*** Least validation loss\n",
      "Train Loss: 2.274, Train Acc: 53.95%\n",
      "Val Loss: 2.388431, Val Acc: 40.67%\n",
      "-------------\n",
      "Epoch 23\n"
     ]
    }
   ],
   "source": [
    "num_bad_epochs = 0\n",
    "epoch = 0\n",
    "least_loss = float('inf')\n",
    "training_stats = pd.DataFrame(columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc'])\n",
    "\n",
    "while(True):\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_loss, train_acc = train_model(model, optim, train_iter, epoch, batch_size, num_classes)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter, batch_size, num_classes) \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    if val_loss < least_loss:\n",
    "        least_loss = val_loss\n",
    "        num_bad_epochs = 0\n",
    "        print(\"*** Least validation loss\")\n",
    "        torch.save(model.state_dict(), \"models/class_15/BiLSTM_BS_32\")\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "#     print(f'Epoch: {epoch+1:2}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%') \n",
    "    print(f'Val Loss: {val_loss:3f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    training_stats = training_stats.append(\n",
    "        pd.Series([epoch+1, train_loss, train_acc, val_loss, val_acc], index=training_stats.columns), \n",
    "        ignore_index=True)\n",
    "    if num_bad_epochs >= 8:\n",
    "        break\n",
    "        \n",
    "    epoch += 1\n",
    "    if epoch == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0181e2758004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_stats/class_15/LSTM_BS_32.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_stats' is not defined"
     ]
    }
   ],
   "source": [
    "training_stats.to_csv('training_stats/class_15/BiLSTM_BS_32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (word_embeddings): Embedding(38110, 100)\n",
       "  (lstm_layer): LSTM(\n",
       "    (lstm): LSTM(100, 100, num_layers=3, batch_first=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (ff_1): Linear(in_features=100, out_features=64, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (ff_2): Linear(in_features=64, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights,num_layers, hidden_size, biDirectional=False)\n",
    "loaded_model.load_state_dict(torch.load('models/class_15/BiLSTM_BS_32'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.384, Test Acc: 41.07\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_model(loaded_model, test_iter, batch_size, num_classes)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(test_sen):\n",
    "    test_sen_list = TEXT.preprocess(test_sen)\n",
    "    print(test_sen_list)\n",
    "    test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
    "\n",
    "    test_sen = np.asarray(test_sen)\n",
    "    test_sen = torch.LongTensor(test_sen)\n",
    "    test_tensor = Variable(test_sen)\n",
    "\n",
    "    loaded_model.eval()\n",
    "    prediction = loaded_model(test_tensor)\n",
    "#     print(\"prediction =\", prediction)\n",
    "\n",
    "    out_class = torch.argmax(prediction)\n",
    "    return out_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_idx = {\n",
    "#     '0' :  '0', # admiration, desire\n",
    "#     '10' :  '1', # disapproval, disgust, disappointment, embarrassment\n",
    "#     '2' : '2',  # anger, annoyance\n",
    "#     '13' : '3', # excitement, amusement\n",
    "#     '18' : '4', # love, caring\n",
    "#     '4' : '5',  # approval\n",
    "#     '15' : '6', # gratitude\n",
    "#     '7' : '7',  # curiosity\n",
    "#     '25' : '8', # sadness , grief, remorse\n",
    "#     '17' : '9', # joy , pride, relief\n",
    "#     '20' : '10', # optimism\n",
    "#     '6' : '11', # confusion\n",
    "#     '22' : '12', # realization\n",
    "#     '26' : '13', # surprise\n",
    "#     '14' :  '14' # fear, nervousness\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "['we', 'need', 'more', 'boards', 'and', 'to', 'create', 'a', 'bit', 'more', 'space', 'for', '[name].', 'then', 'we’ll', 'be', 'good.']\n",
      "prediction = tensor([[9.9853e-01, 3.2671e-10, 5.1728e-11, 7.2792e-10, 1.3660e-03, 9.4108e-16,\n",
      "         1.1968e-07, 6.2128e-13, 1.4500e-19, 1.0028e-18, 1.0177e-04, 6.5502e-21,\n",
      "         5.3880e-22, 2.4030e-21, 1.1633e-22]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0)\n",
      "------------\n",
      "['not', 'surprised,', 'damn', 'that', 'sucks.', 'concussions', 'are', 'awful.']\n",
      "prediction = tensor([[3.2741e-15, 1.0000e+00, 6.7946e-14, 7.6872e-15, 1.7044e-22, 1.6645e-24,\n",
      "         2.4135e-20, 9.7690e-20, 4.1407e-17, 1.1525e-24, 2.9055e-24, 6.3256e-27,\n",
      "         4.9868e-28, 2.8914e-28, 1.8674e-28]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(1)\n",
      "------------\n",
      "['thank', 'you', 'so', 'much!', 'this', 'is', 'so', 'genuine', 'and', 'so', 'helpful.', 'thank', 'you', 'so', 'much', 'for', 'your', 'time', 'and', 'your', 'thoughts.']\n",
      "prediction = tensor([[6.6895e-24, 7.1446e-33, 4.7108e-27, 1.2531e-23, 2.0055e-35, 1.8136e-36,\n",
      "         1.0000e+00, 4.9177e-20, 5.2536e-37, 6.4702e-35, 2.1719e-26, 3.9376e-41,\n",
      "         6.5721e-43, 2.1637e-41, 1.2470e-40]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(6)\n",
      "------------\n",
      "['i', 'am', 'so', 'happy', 'for', 'you']\n",
      "prediction = tensor([[1.0000e+00, 1.2975e-11, 2.1345e-12, 9.8306e-09, 7.5624e-11, 6.7373e-17,\n",
      "         7.0772e-09, 8.0659e-13, 1.0965e-16, 6.8339e-20, 8.8680e-07, 2.4034e-22,\n",
      "         9.9157e-24, 3.9815e-23, 4.4552e-24]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0)\n",
      "------------\n",
      "['i', 'do', 'not', 'know', 'what', 'that', 'is.']\n",
      "prediction = tensor([[4.3660e-06, 8.9185e-02, 9.1072e-01, 1.1118e-05, 2.7419e-13, 1.0040e-10,\n",
      "         4.7645e-08, 4.3246e-05, 3.4240e-05, 3.9207e-11, 4.9222e-08, 7.0486e-13,\n",
      "         5.9140e-14, 1.2035e-13, 3.4629e-14]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(2)\n",
      "------------\n",
      "['are', 'you', 'kidding', 'me!!', 'really??']\n",
      "prediction = tensor([[3.7493e-05, 2.0075e-06, 5.0633e-05, 1.8701e-02, 2.6293e-21, 2.6112e-14,\n",
      "         3.2242e-05, 9.8118e-01, 1.1757e-08, 1.0857e-15, 2.6699e-09, 9.6104e-19,\n",
      "         2.6229e-21, 2.0991e-20, 3.7851e-20]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# test_sen1 = \"I enjoyed it.\"\n",
    "test_sen1 = \"We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\" # Neutral\n",
    "test_sen2 = \"Not surprised, damn that sucks. Concussions are awful.\" # Anger\n",
    "# test_sen3 = \"Are you kidding me!! Really??\"\n",
    "# test_sen3 = \"seriously wtf. I want to see how the whole hand went in detail. that was the sickest soulread ever\" # Anger\n",
    "test_sen3 = \"Thank you SO much! This is so genuine and so helpful. Thank you so much for your time and your thoughts.\"\n",
    "test_sen4 = \"I am so happy for you\" # Joy\n",
    "test_sen5 = \"I do not know what that is.\" # Nuetral\n",
    "test_sen6 = \"Are you kidding me!! Really??\" # Joy\n",
    "\n",
    "test_sen = [test_sen1, test_sen2, test_sen3, test_sen4, test_sen5, test_sen6]\n",
    "\n",
    "for i in range(6):\n",
    "    print('------------')\n",
    "    x = test_sentence(test_sen[i])\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
