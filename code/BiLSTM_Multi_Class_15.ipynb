{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torch.optim as optim\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size = 100, biDirectional = False):\n",
    "        super(Model, self).__init__() \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        num_classes : 28 = (For full classification)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.mlp_out_size = mlp_out_size\n",
    "        self.biDirectional = biDirectional\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        \n",
    "        self.lstm_layer = LSTM(self.batch_size, self.hidden_size, self.embedding_length, self.biDirectional, self.num_layers)\n",
    "\n",
    "        if(self.biDirectional):\n",
    "            self.mlp = MLP(self.hidden_size*2, self.mlp_out_size, self.num_classes)\n",
    "#             self.FF = nn.Linear(self.hidden_size*2, num_classes)\n",
    "        else:\n",
    "            self.mlp = MLP(self.hidden_size, self.mlp_out_size, self.num_classes)\n",
    "#             self.FF = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        input_ = self.word_embeddings(input_sequence)\n",
    "        out_lstm, final_hidden_state = self.lstm_layer(input_)\n",
    "        if self.biDirectional:\n",
    "            final_hidden_state = final_hidden_state.view(self.num_layers, 2, input_.shape[0], self.hidden_size) # num_layer x num_dir x batch x hidden\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "            final_hidden_state = final_hidden_state.transpose(0,1).reshape(input_.shape[0], self.hidden_size*2)\n",
    "        else:\n",
    "            final_hidden_state = final_hidden_state[-1]\n",
    "        \n",
    "        mlp_output = self.mlp(final_hidden_state)\n",
    "#         ff_output = self.FF(mlp_output)\n",
    "#         print(\"FF out size: \", ff_output.shape)\n",
    "        predictions = torch.softmax(mlp_output, dim = -1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.biDirectional= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "    def forward(self, input_sequence, batch_size=None):\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "        return out_lstm, final_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If want to add extra MLP Layer\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim, self.num_classes)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "#         out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, train_iter, epoch, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    \n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.labels\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size): # One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction = model(text)\n",
    "#         print(prediction.shape)\n",
    "#         print(prediction)\n",
    "#         print(target.shape)\n",
    "#         print(target)\n",
    "        loss =  loss_fn(prediction, target)\n",
    "#         if math.isnan(loss.item()):\n",
    "#             print(prediction, target)\n",
    "\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_iter, batch_size, num_classes):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not batch_size):\n",
    "                continue\n",
    "            target = batch.labels\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            # Sanity check\n",
    "            # print(\"Test Prediction: \", prediction)\n",
    "            # Defualt - Cross entropy loss funtion\n",
    "            loss =  loss_fn(prediction, target)\n",
    "            \n",
    "            if math.isnan(loss.item()):\n",
    "                print(prediction, target)\n",
    "            \n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "            \n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size= 16, embedding_length = 100):\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=30)\n",
    "    LABELS = data.LabelField(batch_first=True, dtype=torch.float)\n",
    "\n",
    "    train, val, test = data.TabularDataset.splits(\n",
    "      path='/Users/prakruti/Documents/GoEmotions-classification/data/multi_class_15/', train='train.tsv',\n",
    "      validation='dev.tsv', test='test.tsv', format='tsv',\n",
    "      fields=[('text', TEXT), ('labels', LABELS)])\n",
    "    \n",
    "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "      (train, val, test), batch_sizes=(batch_size, batch_size, batch_size), sort_key=lambda x: len(x.text), device=0)\n",
    "\n",
    "    # build the vocabulary\n",
    "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
    "    LABELS.build_vocab(train)\n",
    "    print(LABELS.vocab.__dict__)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({'0': 4771, '1': 4387, '2': 4037, '3': 3181, '4': 3173, '5': 2939, '6': 2662, '7': 2191, '8': 1948, '9': 1716, '10': 1581, '11': 1368, '12': 1110, '13': 1060, '14': 760}), 'itos': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'], 'unk_index': None, 'stoi': defaultdict(None, {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14}), 'vectors': None}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_data(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab_size, 'models/class_15/BiLSTM_vocab')\n",
    "torch.save(word_embeddings, 'models/class_15/BiLSTM_word_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "learning_rate = 2e-4\n",
    "embedding_length = 100\n",
    "num_classes = 15\n",
    "mlp_out_size = 64\n",
    "weights = word_embeddings\n",
    "hidden_size = 100\n",
    "num_layers = 4\n",
    "\n",
    "model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, num_layers, hidden_size, biDirectional=True)\n",
    "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch: 01\n",
      "*** Least validation loss\n",
      "Train Loss: 2.664, Train Acc: 12.55%\n",
      "Val Loss: 2.646019, Val Acc: 12.18%\n",
      "-------------\n",
      "Epoch 1\n",
      "Epoch: 02\n",
      "*** Least validation loss\n",
      "Train Loss: 2.647, Train Acc: 15.27%\n",
      "Val Loss: 2.610644, Val Acc: 17.03%\n",
      "-------------\n",
      "Epoch 2\n",
      "Epoch: 03\n",
      "*** Least validation loss\n",
      "Train Loss: 2.604, Train Acc: 20.43%\n",
      "Val Loss: 2.585395, Val Acc: 20.84%\n",
      "-------------\n",
      "Epoch 3\n",
      "Epoch: 04\n",
      "*** Least validation loss\n",
      "Train Loss: 2.571, Train Acc: 23.93%\n",
      "Val Loss: 2.561994, Val Acc: 23.19%\n",
      "-------------\n",
      "Epoch 4\n",
      "Epoch: 05\n",
      "*** Least validation loss\n",
      "Train Loss: 2.536, Train Acc: 27.55%\n",
      "Val Loss: 2.525132, Val Acc: 26.98%\n",
      "-------------\n",
      "Epoch 5\n",
      "Epoch: 06\n",
      "*** Least validation loss\n",
      "Train Loss: 2.505, Train Acc: 30.69%\n",
      "Val Loss: 2.508696, Val Acc: 28.49%\n",
      "-------------\n",
      "Epoch 6\n",
      "Epoch: 07\n",
      "*** Least validation loss\n",
      "Train Loss: 2.464, Train Acc: 34.82%\n",
      "Val Loss: 2.482622, Val Acc: 31.19%\n",
      "-------------\n",
      "Epoch 7\n",
      "Epoch: 08\n",
      "*** Least validation loss\n",
      "Train Loss: 2.444, Train Acc: 36.84%\n",
      "Val Loss: 2.478232, Val Acc: 31.79%\n",
      "-------------\n",
      "Epoch 8\n",
      "Epoch: 09\n",
      "*** Least validation loss\n",
      "Train Loss: 2.429, Train Acc: 38.43%\n",
      "Val Loss: 2.457933, Val Acc: 33.71%\n",
      "-------------\n",
      "Epoch 9\n",
      "Epoch: 10\n",
      "Train Loss: 2.420, Train Acc: 39.28%\n",
      "Val Loss: 2.466111, Val Acc: 32.89%\n",
      "-------------\n",
      "Epoch 10\n",
      "Epoch: 11\n",
      "*** Least validation loss\n",
      "Train Loss: 2.408, Train Acc: 40.49%\n",
      "Val Loss: 2.448159, Val Acc: 34.76%\n",
      "-------------\n",
      "Epoch 11\n",
      "Epoch: 12\n",
      "*** Least validation loss\n",
      "Train Loss: 2.398, Train Acc: 41.50%\n",
      "Val Loss: 2.441724, Val Acc: 35.26%\n",
      "-------------\n",
      "Epoch 12\n",
      "Epoch: 13\n",
      "*** Least validation loss\n",
      "Train Loss: 2.389, Train Acc: 42.27%\n",
      "Val Loss: 2.437398, Val Acc: 35.62%\n",
      "-------------\n",
      "Epoch 13\n",
      "Epoch: 14\n",
      "*** Least validation loss\n",
      "Train Loss: 2.382, Train Acc: 43.05%\n",
      "Val Loss: 2.433695, Val Acc: 36.27%\n",
      "-------------\n",
      "Epoch 14\n",
      "Epoch: 15\n",
      "*** Least validation loss\n",
      "Train Loss: 2.372, Train Acc: 44.00%\n",
      "Val Loss: 2.432823, Val Acc: 36.12%\n",
      "-------------\n",
      "Epoch 15\n",
      "Epoch: 16\n",
      "*** Least validation loss\n",
      "Train Loss: 2.364, Train Acc: 44.82%\n",
      "Val Loss: 2.428795, Val Acc: 36.59%\n",
      "-------------\n",
      "Epoch 16\n",
      "Epoch: 17\n",
      "*** Least validation loss\n",
      "Train Loss: 2.355, Train Acc: 45.83%\n",
      "Val Loss: 2.425240, Val Acc: 36.98%\n",
      "-------------\n",
      "Epoch 17\n",
      "Epoch: 18\n",
      "*** Least validation loss\n",
      "Train Loss: 2.345, Train Acc: 46.78%\n",
      "Val Loss: 2.417923, Val Acc: 37.78%\n",
      "-------------\n",
      "Epoch 18\n",
      "Epoch: 19\n",
      "*** Least validation loss\n",
      "Train Loss: 2.338, Train Acc: 47.51%\n",
      "Val Loss: 2.416693, Val Acc: 38.00%\n",
      "-------------\n",
      "Epoch 19\n",
      "Epoch: 20\n",
      "*** Least validation loss\n",
      "Train Loss: 2.336, Train Acc: 47.66%\n",
      "Val Loss: 2.412874, Val Acc: 38.30%\n",
      "-------------\n",
      "Epoch 20\n",
      "Epoch: 21\n",
      "Train Loss: 2.332, Train Acc: 48.15%\n",
      "Val Loss: 2.421236, Val Acc: 37.44%\n",
      "-------------\n",
      "Epoch 21\n",
      "Epoch: 22\n",
      "Train Loss: 2.327, Train Acc: 48.59%\n",
      "Val Loss: 2.413986, Val Acc: 38.15%\n",
      "-------------\n",
      "Epoch 22\n",
      "Epoch: 23\n",
      "Train Loss: 2.326, Train Acc: 48.77%\n",
      "Val Loss: 2.414134, Val Acc: 38.12%\n",
      "-------------\n",
      "Epoch 23\n",
      "Epoch: 24\n",
      "*** Least validation loss\n",
      "Train Loss: 2.321, Train Acc: 49.23%\n",
      "Val Loss: 2.407289, Val Acc: 38.81%\n",
      "-------------\n",
      "Epoch 24\n",
      "Epoch: 25\n",
      "Train Loss: 2.318, Train Acc: 49.52%\n",
      "Val Loss: 2.412634, Val Acc: 38.34%\n",
      "-------------\n",
      "Epoch 25\n",
      "Epoch: 26\n",
      "Train Loss: 2.315, Train Acc: 49.80%\n",
      "Val Loss: 2.417149, Val Acc: 37.91%\n",
      "-------------\n",
      "Epoch 26\n",
      "Epoch: 27\n",
      "Train Loss: 2.312, Train Acc: 50.20%\n",
      "Val Loss: 2.408703, Val Acc: 38.75%\n",
      "-------------\n",
      "Epoch 27\n",
      "Epoch: 28\n",
      "*** Least validation loss\n",
      "Train Loss: 2.308, Train Acc: 50.51%\n",
      "Val Loss: 2.405847, Val Acc: 39.01%\n",
      "-------------\n",
      "Epoch 28\n",
      "Epoch: 29\n",
      "Train Loss: 2.304, Train Acc: 50.93%\n",
      "Val Loss: 2.406922, Val Acc: 38.92%\n",
      "-------------\n",
      "Epoch 29\n",
      "Epoch: 30\n",
      "Train Loss: 2.304, Train Acc: 50.96%\n",
      "Val Loss: 2.412398, Val Acc: 38.19%\n",
      "-------------\n",
      "Epoch 30\n",
      "Epoch: 31\n",
      "Train Loss: 2.300, Train Acc: 51.34%\n",
      "Val Loss: 2.406975, Val Acc: 38.92%\n",
      "-------------\n",
      "Epoch 31\n",
      "Epoch: 32\n",
      "Train Loss: 2.300, Train Acc: 51.30%\n",
      "Val Loss: 2.409722, Val Acc: 38.64%\n",
      "-------------\n",
      "Epoch 32\n",
      "Epoch: 33\n",
      "Train Loss: 2.296, Train Acc: 51.75%\n",
      "Val Loss: 2.406052, Val Acc: 39.05%\n",
      "-------------\n",
      "Epoch 33\n",
      "Epoch: 34\n",
      "Train Loss: 2.293, Train Acc: 52.05%\n",
      "Val Loss: 2.417354, Val Acc: 37.67%\n",
      "-------------\n",
      "Epoch 34\n",
      "Epoch: 35\n",
      "Train Loss: 2.292, Train Acc: 52.12%\n",
      "Val Loss: 2.413456, Val Acc: 38.38%\n",
      "-------------\n",
      "Epoch 35\n",
      "Epoch: 36\n",
      "*** Least validation loss\n",
      "Train Loss: 2.288, Train Acc: 52.55%\n",
      "Val Loss: 2.401888, Val Acc: 39.33%\n",
      "-------------\n",
      "Epoch 36\n",
      "Epoch: 37\n",
      "Train Loss: 2.283, Train Acc: 52.97%\n",
      "Val Loss: 2.408991, Val Acc: 38.56%\n",
      "-------------\n",
      "Epoch 37\n",
      "Epoch: 38\n",
      "Train Loss: 2.280, Train Acc: 53.33%\n",
      "Val Loss: 2.406190, Val Acc: 38.92%\n",
      "-------------\n",
      "Epoch 38\n",
      "Epoch: 39\n",
      "Train Loss: 2.279, Train Acc: 53.43%\n",
      "Val Loss: 2.402666, Val Acc: 39.25%\n",
      "-------------\n",
      "Epoch 39\n",
      "Epoch: 40\n",
      "Train Loss: 2.274, Train Acc: 53.92%\n",
      "Val Loss: 2.411665, Val Acc: 38.28%\n",
      "-------------\n",
      "Epoch 40\n",
      "Epoch: 41\n",
      "Train Loss: 2.270, Train Acc: 54.35%\n",
      "Val Loss: 2.404676, Val Acc: 39.05%\n",
      "-------------\n",
      "Epoch 41\n",
      "Epoch: 42\n",
      "Train Loss: 2.264, Train Acc: 54.96%\n",
      "Val Loss: 2.407821, Val Acc: 38.75%\n",
      "-------------\n",
      "Epoch 42\n",
      "Epoch: 43\n",
      "Train Loss: 2.259, Train Acc: 55.36%\n",
      "Val Loss: 2.409284, Val Acc: 38.45%\n",
      "-------------\n",
      "Epoch 43\n",
      "Epoch: 44\n",
      "*** Least validation loss\n",
      "Train Loss: 2.259, Train Acc: 55.48%\n",
      "Val Loss: 2.400747, Val Acc: 39.46%\n",
      "-------------\n",
      "Epoch 44\n",
      "Epoch: 45\n",
      "*** Least validation loss\n",
      "Train Loss: 2.254, Train Acc: 55.98%\n",
      "Val Loss: 2.397494, Val Acc: 39.94%\n",
      "-------------\n",
      "Epoch 45\n",
      "Epoch: 46\n",
      "Train Loss: 2.248, Train Acc: 56.58%\n",
      "Val Loss: 2.400961, Val Acc: 39.35%\n",
      "-------------\n",
      "Epoch 46\n",
      "Epoch: 47\n",
      "Train Loss: 2.249, Train Acc: 56.46%\n",
      "Val Loss: 2.399084, Val Acc: 39.66%\n",
      "-------------\n",
      "Epoch 47\n",
      "Epoch: 48\n",
      "Train Loss: 2.246, Train Acc: 56.81%\n",
      "Val Loss: 2.405318, Val Acc: 39.09%\n",
      "-------------\n",
      "Epoch 48\n",
      "Epoch: 49\n",
      "Train Loss: 2.241, Train Acc: 57.22%\n",
      "Val Loss: 2.399567, Val Acc: 39.48%\n",
      "-------------\n",
      "Epoch 49\n",
      "Epoch: 50\n",
      "Train Loss: 2.239, Train Acc: 57.46%\n",
      "Val Loss: 2.400788, Val Acc: 39.40%\n",
      "-------------\n",
      "Epoch 50\n",
      "Epoch: 51\n",
      "*** Least validation loss\n",
      "Train Loss: 2.235, Train Acc: 57.82%\n",
      "Val Loss: 2.392511, Val Acc: 40.43%\n",
      "-------------\n",
      "Epoch 51\n"
     ]
    }
   ],
   "source": [
    "num_bad_epochs = 0\n",
    "epoch = 0\n",
    "least_loss = float('inf')\n",
    "training_stats = pd.DataFrame(columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc'])\n",
    "\n",
    "while(True):\n",
    "    print(\"Epoch\", epoch)\n",
    "    train_loss, train_acc = train_model(model, optim, train_iter, epoch, batch_size, num_classes)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter, batch_size, num_classes) \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    if val_loss < least_loss:\n",
    "        least_loss = val_loss\n",
    "        num_bad_epochs = 0\n",
    "        print(\"*** Least validation loss\")\n",
    "        torch.save(model.state_dict(), \"models/class_15/BiLSTM_BS_32_4L\")\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "#     print(f'Epoch: {epoch+1:2}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%') \n",
    "    print(f'Val Loss: {val_loss:3f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    training_stats = training_stats.append(\n",
    "        pd.Series([epoch+1, train_loss, train_acc, val_loss, val_acc], index=training_stats.columns), \n",
    "        ignore_index=True)\n",
    "    if num_bad_epochs >= 10:\n",
    "        break\n",
    "        \n",
    "    epoch += 1\n",
    "    if epoch == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats.to_csv('training_stats/class_15/BiLSTM_BS_32_4L.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (word_embeddings): Embedding(38110, 100)\n",
       "  (lstm_layer): LSTM(\n",
       "    (lstm): LSTM(100, 100, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (ff_1): Linear(in_features=200, out_features=64, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (ff_2): Linear(in_features=64, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Model(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights,num_layers, hidden_size, biDirectional=True)\n",
    "loaded_model.load_state_dict(torch.load('models/class_15/BiLSTM_BS_32_4L'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.380, Test Acc: 41.33\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_model(loaded_model, test_iter, batch_size, num_classes)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(test_sen):\n",
    "    test_sen_list = TEXT.preprocess(test_sen)\n",
    "    print(test_sen_list)\n",
    "    test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
    "\n",
    "    test_sen = np.asarray(test_sen)\n",
    "    test_sen = torch.LongTensor(test_sen)\n",
    "    test_tensor = Variable(test_sen)\n",
    "\n",
    "    loaded_model.eval()\n",
    "    prediction = loaded_model(test_tensor)\n",
    "#     print(\"prediction =\", prediction)\n",
    "    out_class = torch.argmax(prediction)\n",
    "    return out_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_idx = {\n",
    "#     '0' :  '0', # admiration, desire\n",
    "#     '10' :  '1', # disapproval, disgust, disappointment, embarrassment\n",
    "#     '2' : '2',  # anger, annoyance\n",
    "#     '13' : '3', # excitement, amusement\n",
    "#     '18' : '4', # love, caring\n",
    "#     '4' : '5',  # approval\n",
    "#     '15' : '6', # gratitude\n",
    "#     '7' : '7',  # curiosity\n",
    "#     '25' : '8', # sadness , grief, remorse\n",
    "#     '17' : '9', # joy , pride, relief\n",
    "#     '20' : '10', # optimism\n",
    "#     '6' : '11', # confusion\n",
    "#     '22' : '12', # realization\n",
    "#     '26' : '13', # surprise\n",
    "#     '14' :  '14' # fear, nervousness\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "['we', 'need', 'more', 'boards', 'and', 'to', 'create', 'a', 'bit', 'more', 'space', 'for', '[name].', 'then', 'we’ll', 'be', 'good.']\n",
      "tensor(0)\n",
      "------------\n",
      "['not', 'surprised,', 'damn', 'that', 'sucks.', 'concussions', 'are', 'awful.']\n",
      "tensor(12)\n",
      "------------\n",
      "['seriously', 'wtf.', 'i', 'want', 'to', 'see', 'how', 'the', 'whole', 'hand', 'went', 'in', 'detail.', 'that', 'was', 'the', 'sickest', 'soulread', 'ever']\n",
      "tensor(2)\n",
      "------------\n",
      "['i', 'am', 'so', 'happy', 'for', 'you']\n",
      "tensor(9)\n",
      "------------\n",
      "['i', 'do', 'not', 'know', 'what', 'that', 'is.']\n",
      "tensor(2)\n",
      "------------\n",
      "['lets', 'do', 'it.', 'come', 'on.']\n",
      "tensor(12)\n"
     ]
    }
   ],
   "source": [
    "# test_sen1 = \"I enjoyed it.\"\n",
    "test_sen1 = \"We need more boards and to create a bit more space for [NAME]. Then we’ll be good.\" # Neutral\n",
    "test_sen2 = \"Not surprised, damn that sucks. Concussions are awful.\" # Anger\n",
    "# test_sen3 = \"Are you kidding me!! Really??\"\n",
    "test_sen3 = \"seriously wtf. I want to see how the whole hand went in detail. that was the sickest soulread ever\" # Anger\n",
    "# test_sen3 = \"Thank you SO much! This is so genuine and so helpful. Thank you so much for your time and your thoughts.\"\n",
    "test_sen4 = \"I am so happy for you\" # Joy\n",
    "test_sen5 = \"I do not know what that is.\" # Nuetral\n",
    "# test_sen6 = \"Are you kidding me!! Really??\" # Joy\n",
    "test_sen6 = \"Lets do it. Come on.\"\n",
    "\n",
    "test_sen = [test_sen1, test_sen2, test_sen3, test_sen4, test_sen5, test_sen6]\n",
    "\n",
    "for i in range(6):\n",
    "    print('------------')\n",
    "    x = test_sentence(test_sen[i])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
